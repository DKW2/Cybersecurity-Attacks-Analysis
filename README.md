# Cybersecurity-Attacks-Analysis
This project is my capstone project for UC Berkeley's MLAI bootcamp. For this project, I needed to come up with my own research question, find the data, and conduct modeling/analysis to answer my question. For this project, I will be analyzing a dataset of network packets and identifying factors which determine if the packet is a cyberattack or not. The goal for the project is to train a classification model that will be able to accurately predict if a packet is normal or malign. The full work and report can be found on this [notebook](https://github.com/DKW2/Cybersecurity-Attacks-Analysis/blob/main/Cybersecurity%20Attacks%20Analysis.ipynb).

# Report
The dataset we're using comes from Kaggle. The data is a synthetic dataset created by "Cyber Range Lab of the Australian Centre for Cyber Security (ACCS) for generating a hybrid of real modern normal activities and synthetic contemporary attack behaviours". In total, the data has 2 million records. However, they graciously sampled the dataset and created a training and testing set of a total of 250,000 records. Thus, instead of utilizing the 2 million records, I used their train/test sets to lower the computation power. Starting with the training set, I first checked to see that the two classes are slightly unbalanced with a split of around 33-66 (normal, malign). I then proceeded to explore the data, taking note of categorical and numerical features as well as indentifying potential features to manipulate/remove. The features were all cleaned very well, so there wasn't much that needed to be done on my end. One feature that had to be removed was the "id" feature, since it's not a feature of the packet.

Once I cleaned the data, I first created a baseline model to have a threshold for my models to beat. The model would always output "malign", resulting in a baseline accuracy of around 55% and recall of 100%. Once the baseline was established, I began training the classification models. For this project, I trained 4 different types of classification models: K-Nearest Neighbors (KNN), Logistic Regression, Support Vector Machines (SVC), and Decision Trees. For the first run, I trained all four models and optimized each one with hyperparameter searching. The scoring metric I chose was recall, since want to maximize the amount of malign packets we can identify. It's fine if we mistake a normal packet for a malign packet since we're just being careful. However, if we identify a malign packet as normal, there could be consequences. One small hiccup, however, was that training a decision tree using recall results in a model that is identical to the baseline model since both have a recall of 100%. Thus, it's more optimal to train the decision tree using accuracy instead. That's why in the following results, there will be 2 decision trees:

![](images/optimalModels.png)

Impressively enough, all of our optimized models had a recall of 97% and higher while the accuracy is around 80%. However, it's difficult to compare the models to the baseline with a recall of 100%. In addition, this doesn't give a very clear picture on which model performed the best. Thus, for a clearer depiction, we'll use Precision-Recall curves to get a better picture of how well each model did. For each model, the closer the curve is to the top right corner, the better the model:

![](images/precisionRecall.png)

As we can see, KNN performed the best with Logistic Regression and SVC performing slightly worse than it.

Now that we have trained our optimized models, I proceeded to examine each model for overlapping features that strongly impacted the model's predictions. This meant examining the branches in the decision tree, searching through the coefficients in the Logistic Regression model, and using permutation_importance to find the most important features. After I analyzed all 4 models, we can definitely see some stand-out impactful features that overlapped in all of them. These all involve either the packet details or the protocol used to send them. Below are some of the findings:
* We found that "Time to live value" is very impactful in determining whether a packet is normal or malign. Lower values lead to normal packets while higher values point toward malignant packets.
* High values in features dealing with packet size and limit such as TCP window advertisement value, bits per second, mean transmited packet size have a strong indication of a normal packet. This is probably because they imply a stable and secure connection between two parties rather than a sneaky method of injecting a malign packet.
* Protocol is also a very strong factor in determining a packet's validity. Packets that utilize popular protocols such as TCP, UDP, and FTP strongly suggests that the packets are normal. This is probably due to the amount of scrutinance and work put into these protocols to make them as secure and safe as possible. Meanwhile, packets that utilize less frequently used protocols have a higher rate of being a malign packet. This is probably due to there being more security holes and supervision when using these protocols.

In essense, we can determine if a packet is normal or malignant by examining the packet transmission details and utilized protocol services. Most malignant packets utilize less popular and secure protocols while normal packets utilize popular protocols such as TCP and UDP. In addition, malignant packets usually have longer life time on the internet and are smaller than normal packets.

Thus, through this project, I trained classification models that can accurately identify malignant packets while also discovering strong features that indicate whether a network packet is normal or malign.

# Summary

With a dataset of network packets collected by the Australian Centre for Cyber Security, we trained multiple classification models to determine whether a network packet was normal or malign. Once trained, all of the models had pretty high performance, sporting 80% accuracy and around 97% recall. After training the models, we examined each model to find features that strongly impacted the models in their predictions. These features are strong indicators for normal or malign packets, allowing us to gain some insight on how these models work. From our analysis, we deduced that the strongest indicators for a network packet are the packet transmission details and utilized service/protocol. Malign packets and normal packets seem to utilize different protocols when being sent. In addition, while malign packets are usually small in size and try to last on the network for as long as possible, normal packets tend to be larger in size and are transferred more securely.

# Next Steps / Recommmendations

With such a high recall and accuracy for every model, a next step would be to utilize these classification models in cybersecurity tools that identify malign packets. This way, people can be more protected against malignant agents trying to steal their information or assets. Another way is just to inform cybersecurity experts about these results, discussing which features are strong indicators and seeking what their opinion is on the matter. This creates discussion and spreads discourse about identifying network packets, which will keep people's digital lives safer in the long run.

In addition, although our models performed well, it's worth using data provided by other sources to confirm/reject the current results we've gotten. The dataset we used for this project is synthetic, meaning that not all of it is real data. This raises the concern that perhaps the data might be completely reflective of real life network packets. Thus, it's worth double checking by utilizing other data.

Besides that, another next step is focusing on creating more accurate models. Currently, Logistic Regression and SVC take a really long time to fit (even with all my computer's processors working in tandam). As such, it's just not feasible to conduct deep hyperparameter searches for these models. However, from our analysis, we've confirmed that many features simply don't have any impact on the model. Thus, if we were to trim the dataset to only include the impactful features, it will be easier to conduct deeper hyperparameter searches for our models. In addition, we can utilize more advanced models such as Random Forests to get better classification models.

For recommendations, I would suggest people to use popular network protocols when transferring data. It's only when you start using less used protocols or access less secure sites/downloads when you begin to run across malignant network packets. For general folk, it would probably boil down to just being aware of the dangers of receiving suspicious advertisements or download links and not accessing insecure web sites.

# Reflection

There are a few things that I would have liked to have done better.

First and foremost was to check the validity and usability of the data with my own eyes rather than other people's opinion. The first dataset I found seemed like a perfect find that contained enough data and features to conduct good analysis. However, as I dove deeper into the dataset, I noticed that all of the models were performing as well as the baseline model. Further testing led to me realizing that the dataset I used was so synthetic that the distribution of features for each label were completely the same. So there was no insight or information to be gained. I wasted so much time and effort on this one dataset and grew frustrated. However, I was able to find this current dataset, which provided me with a lot more usable data and insight to complete the project. Thus, a lesson for me to learn from.

Second, I think I could have done more during my model training. As I mentioned in my next steps, I could optimize the dataset and conduct deeper searches for the models to get a higher accuracy and recall score. I also think that the range of some of the hyperparameters I was searching for could be expanded as well.

Lastly, I feel like I could explain the features a bit better or translate their names into more readable/clearer titles. Most of it is internet protocol terminology, so it could get confusing for a layman to understand what each feature represents. Perhaps I should have added the entire features description as a section in this report?
